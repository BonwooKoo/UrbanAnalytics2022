select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value)
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value) %>%
mutate(test = t.test(first, second)$statistic)
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value)
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value) %>% with(unlist(.$first))
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value) %>% with(.$first)
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value) %>% with(unlist(.$first))
compare_df %>%
select(type, building, sky, tree, road, sidewalk) %>%
st_drop_geometry() %>%
group_by(type) %>%
summarise(across(c(building, sky, tree, road, sidewalk), list)) %>%
pivot_longer(cols = building:sidewalk) %>%
pivot_wider(names_from=type, values_from=value) %>% with(.$first)
here()
library(here)
# Package names
packages <- c("rtweet", "ggplot2", "dplyr", "tidytext", "tidyverse", "igraph", "ggraph", "tidyr", "wordcloud2", "textdata")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
# Package names
packages <- c("rtweet", "ggplot2", "dplyr", "tidytext", "tidyverse", "igraph", "ggraph", "tidyr", "wordcloud2", "textdata")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
# whatever name you assigned to your created app
appname <- "UrbanAnalytics_tutorial"
# create token named "twitter_token"
# the keys used should be replaced by your own keys obtained by creating the app
twitter_token <- create_token(
app = appname,
consumer_key = Sys.getenv("twitter_key"),
consumer_secret = Sys.getenv("twitter_key_secret"),
access_token = Sys.getenv("twitter_access_token"),
access_secret = Sys.getenv("twitter_access_token_secret"))
#get/search tweets with any words and hashtags
#as an example 'piedmont park' is used which is save under the data frame name - 'park_tweets'
#'search_tweets'- is the function of 'rtweet' library
#q is the words/hashtags used to search tweets; n is the number of tweets to download at a time. Here it is 100.
gt_tweets_all <- search_tweets(q = "Georgia Tech", n = 200)
# When were these tweet made?
gt_tweets_all %>%
ggplot(data = .) +
geom_histogram(mapping = aes(x = created_at), color="gray") +
scale_x_datetime(date_labels = " %H:%M on %b %d")
#For something more managable:
gt_tweets <- gt_tweets_all[, sapply(gt_tweets_all, Negate(anyNA))]
#show headers of the tweets downloaded from piedmont park
head(gt_tweets, n = 5)
#unique screen_name and location of those who tweeted about Georga Tech
users <- gt_tweets %>%
select(screen_name, location) %>%
group_by(screen_name) %>%
summarise(location = first(location))
gt_tweets
obama <- rtweet::get_timeline("BarackObama", n = 3200)
biden <- rtweet::get_timeline("JoeBiden", n=3200)
tweets <- bind_rows(
obama %>% filter(is_retweet==F) %>% select(text, screen_name, created_at, retweet_count, favorite_count),
biden %>% filter(is_retweet==F) %>% select(text, screen_name, created_at, retweet_count, favorite_count)
)
obama
obama
# Package names
packages <- c("rtweet", "ggplot2", "dplyr", "tidytext", "tidyverse", "igraph", "ggraph", "tidyr", "wordcloud2", "textdata")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
# whatever name you assigned to your created app
appname <- "UrbanAnalytics_tutorial"
# create token named "twitter_token"
# the keys used should be replaced by your own keys obtained by creating the app
twitter_token <- create_token(
app = appname,
consumer_key = Sys.getenv("twitter_key"),
consumer_secret = Sys.getenv("twitter_key_secret"),
access_token = Sys.getenv("twitter_access_token"),
access_secret = Sys.getenv("twitter_access_token_secret"))
#get/search tweets with any words and hashtags
#as an example 'piedmont park' is used which is save under the data frame name - 'park_tweets'
#'search_tweets'- is the function of 'rtweet' library
#q is the words/hashtags used to search tweets; n is the number of tweets to download at a time. Here it is 100.
park_tweets_all <- search_tweets(q = "Piedmont Park", n = 200)
park_tweets_all <- bind_cols(park_tweets_all,
users_data(park_tweets_all) %>% select(screen_name, location))
# When were these tweet made?
park_tweets_all %>%
ggplot(data = .) +
geom_histogram(mapping = aes(x = created_at), color="gray") +
scale_x_datetime(date_labels = " %H:%M on %b %d")
#unique screen_name and location of those who tweeted about Georga Tech
users <- park_tweets_all %>%
select(screen_name, location) %>%
group_by(screen_name) %>%
summarise(location = first(location))
user_loc <- users$location %>%
table() %>%
as_tibble()
names(user_loc) <- c("location", "n")
loc_word <- user_loc %>%
unnest_tokens(input = location, # name of column in the input data
output = Location) %>% # name of the column that will be in the output
group_by(Location) %>%
summarize(n = sum(n)) %>%
arrange(desc(n))
loc_word <- loc_word %>%
filter(n > 1 & Location != "")
loc_word %>%
mutate(Location = factor(Location, levels = Location)) %>%
ggplot(data = .) +
geom_col(mapping = aes(x = Location, y = n)) +
coord_flip()
my_twts <- search_tweets(q = "##Netflix", n = 200,
lang = "en",
include_rts = FALSE)
#save the tweets downloaded using rtweet as a csv
#write_as_csv(my_twts, "my_twts.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
head(my_twts$text,3)
my_twts
# Package names
packages <- c("rtweet", "ggplot2", "dplyr", "tidytext", "tidyverse", "igraph", "ggraph", "tidyr", "wordcloud2", "textdata")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
# whatever name you assigned to your created app
appname <- "UrbanAnalytics_tutorial"
# create token named "twitter_token"
# the keys used should be replaced by your own keys obtained by creating the app
twitter_token <- create_token(
app = appname,
consumer_key = Sys.getenv("twitter_key"),
consumer_secret = Sys.getenv("twitter_key_secret"),
access_token = Sys.getenv("twitter_access_token"),
access_secret = Sys.getenv("twitter_access_token_secret"))
#get/search tweets with any words and hashtags
#as an example 'piedmont park' is used which is save under the data frame name - 'park_tweets'
#'search_tweets'- is the function of 'rtweet' library
#q is the words/hashtags used to search tweets; n is the number of tweets to download at a time. Here it is 100.
park_tweets_all <- search_tweets(q = "Piedmont Park", n = 200)
park_tweets_all <- bind_cols(park_tweets_all,
users_data(park_tweets_all) %>% select(screen_name, location))
# When were these tweet made?
park_tweets_all %>%
ggplot(data = .) +
geom_histogram(mapping = aes(x = created_at), color="gray") +
scale_x_datetime(date_labels = " %H:%M on %b %d")
#unique screen_name and location of those who tweeted about Georga Tech
users <- park_tweets_all %>%
select(screen_name, location) %>%
group_by(screen_name) %>%
summarise(location = first(location))
user_loc <- users$location %>%
table() %>%
as_tibble()
names(user_loc) <- c("location", "n")
loc_word <- user_loc %>%
unnest_tokens(input = location, # name of column in the input data
output = Location) %>% # name of the column that will be in the output
group_by(Location) %>%
summarize(n = sum(n)) %>%
arrange(desc(n))
loc_word <- loc_word %>%
filter(n > 1 & Location != "")
loc_word %>%
mutate(Location = factor(Location, levels = Location)) %>%
ggplot(data = .) +
geom_col(mapping = aes(x = Location, y = n)) +
coord_flip()
# Get time lines
obama <- rtweet::get_timeline("BarackObama", n = 3200)
biden <- rtweet::get_timeline("JoeBiden", n=3200)
# Add screen nam
obama <- bind_cols(obama,
users_data(obama) %>% select(screen_name, location))
biden <- bind_cols(biden,
users_data(biden) %>% select(screen_name, location))
# Row-bind the two
tweets <- bind_rows(
obama %>% select(text, screen_name, created_at, retweet_count, favorite_count),
biden %>% select(text, screen_name, created_at, retweet_count, favorite_count)
)
my_twts
# Package names
packages <- c("rtweet", "ggplot2", "dplyr", "tidytext", "tidyverse", "igraph", "ggraph", "tidyr", "wordcloud2", "textdata")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Load packages
invisible(lapply(packages, library, character.only = TRUE))
# whatever name you assigned to your created app
appname <- "UrbanAnalytics_tutorial"
# create token named "twitter_token"
# the keys used should be replaced by your own keys obtained by creating the app
twitter_token <- create_token(
app = appname,
consumer_key = Sys.getenv("twitter_key"),
consumer_secret = Sys.getenv("twitter_key_secret"),
access_token = Sys.getenv("twitter_access_token"),
access_secret = Sys.getenv("twitter_access_token_secret"))
#get/search tweets with any words and hashtags
#as an example 'piedmont park' is used which is save under the data frame name - 'park_tweets'
#'search_tweets'- is the function of 'rtweet' library
#q is the words/hashtags used to search tweets; n is the number of tweets to download at a time. Here it is 100.
park_tweets_all <- search_tweets(q = "Piedmont Park", n = 200)
park_tweets_all <- bind_cols(park_tweets_all,
users_data(park_tweets_all) %>% select(screen_name, location))
# When were these tweet made?
park_tweets_all %>%
ggplot(data = .) +
geom_histogram(mapping = aes(x = created_at), color="gray") +
scale_x_datetime(date_labels = " %H:%M on %b %d")
#unique screen_name and location of those who tweeted about Georga Tech
users <- park_tweets_all %>%
select(screen_name, location) %>%
group_by(screen_name) %>%
summarise(location = first(location))
user_loc <- users$location %>%
table() %>%
as_tibble()
names(user_loc) <- c("location", "n")
loc_word <- user_loc %>%
unnest_tokens(input = location, # name of column in the input data
output = Location) %>% # name of the column that will be in the output
group_by(Location) %>%
summarize(n = sum(n)) %>%
arrange(desc(n))
loc_word <- loc_word %>%
filter(n > 1 & Location != "")
loc_word %>%
mutate(Location = factor(Location, levels = Location)) %>%
ggplot(data = .) +
geom_col(mapping = aes(x = Location, y = n)) +
coord_flip()
# Get time lines
obama <- rtweet::get_timeline("BarackObama", n = 3200)
biden <- rtweet::get_timeline("JoeBiden", n=3200)
# Add screen nam
obama <- bind_cols(obama,
users_data(obama) %>% select(screen_name, location))
biden <- bind_cols(biden,
users_data(biden) %>% select(screen_name, location))
# Row-bind the two
tweets <- bind_rows(
obama %>% select(text, screen_name, created_at, retweet_count, favorite_count),
biden %>% select(text, screen_name, created_at, retweet_count, favorite_count)
)
ggplot(tweets, aes(x = created_at, fill = screen_name)) +
geom_histogram(position = "identity", bins = 50, show.legend = FALSE) +
facet_wrap(~screen_name, ncol = 1) +
ggtitle("Tweet Activity")
# Regex that matches URL-type string
replace_reg <- "http[s]?://[A-Za-z\\d/\\.]+|&amp;|&lt;|&gt;"
# Tidy the string
tidy_tweets_words <- tweets %>%
# Drop retweets
filter(!str_detect(text, "^RT")) %>%
# Drop URLs
mutate(text = str_replace_all(text, replace_reg, "")) %>%
# Add id column
mutate(id = row_number()) %>%
# Tokenization (word tokens)
unnest_tokens(
word, text, token = "words") %>%
# Drop Stopwords & drop non-alphabet-only strings
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
tidy_tweets_words
frequency <- tidy_tweets_words %>%
group_by(screen_name) %>%
count(word, sort = TRUE) %>%
left_join(tidy_tweets_words %>%
group_by(screen_name) %>%
summarise(total = n())) %>%
mutate(freq = n/total)
frequency_wide <- frequency %>%
select(screen_name, word, freq) %>%
pivot_wider(names_from = screen_name, values_from = freq) %>%
arrange(desc(BarackObama), desc(JoeBiden))
ggplot(frequency_wide, mapping = aes(x = BarackObama, y = JoeBiden)) +
geom_jitter(
alpha = 0.1, size = 2.5, width = 0.15, height = 0.15) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 0) +
scale_x_log10(labels = scales::percent_format()) +
scale_y_log10(labels = scales::percent_format()) +
geom_abline(color = "red") + theme_bw()
my_twts <- search_tweets(q = "##Netflix", n = 200,
lang = "en",
include_rts = FALSE)
#save the tweets downloaded using rtweet as a csv
#write_as_csv(my_twts, "my_twts.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
head(my_twts$text,3)
my_twts$cleanedTxt <- gsub("http.*","",  my_twts$text)
my_twts$cleanedTxt <- gsub("https.*","", my_twts$cleanedTxt)
my_twts$cleanedTxt <- gsub("&amp*","", my_twts$cleanedTxt)
head(my_twts$cleanedTxt, 3)
# We first remove punctuation, convert to lowercase, add id for each tweet!
my_twts_clean <- my_twts %>%
dplyr::select(cleanedTxt) %>%
unnest_tokens(output = word, input = cleanedTxt)
#Then we will check the number of rows after tokenization
nrow(my_twts_clean)
View(my_twts_clean)
# plot the top 15 words and sort them in order of their counts
my_twts_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "words",
y = "counts",
title = "Figure 2: Unique wordcounts found in tweets, with no stop words")
#-- Do you observe any problem?
# load list of stop words - from the tidytext package
data("stop_words")
# view first 6 words
#head(stop_words)
#View(stop_words)
# remove stop words from your list of words
cleanTokens <- my_twts_clean %>% anti_join(stop_words)
# Check the number of rows after removal of the stop words. There should be fewer words now
nrow(cleanTokens)
# plot the top 10 words -- notice any issues?
cleanTokens %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "count",
x = "words",
title = "Figure 3: Unique wordcounts found in tweets after applying stop words",
subtitle = "Stop words removed from the list")
###You may need these
#library(wordcloud)
library(RColorBrewer)
pal <- brewer.pal(8,"Dark2")
#Get some frequency counts for each word
freq_df1 <- cleanTokens %>%
count(word, sort = TRUE) %>%
top_n(100) %>%
mutate(word = reorder(word, n))
wordcloud2(data = freq_df1, minRotation = 0, maxRotation = 0, ellipticity = 0.8)
###FURTHER CLEANING BASED ON YOUR EXPERTISE! YOUR CODE HERE
my_stopwords <- data.frame(c(stop_words$word, 'netflix', '&amp', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',
'10', '11', '12', '13', '14', '15', '16', '17', '19', '27', '37', '110', '99', '50', '30', '200'))
colnames(my_stopwords) <- "word"
cleanTokens2 <- my_twts_clean %>%
anti_join(my_stopwords)
####Rerun the freq counts
freq_df2 <- cleanTokens2 %>%
count(word, sort = TRUE) %>%
top_n(100) %>%
mutate(word = reorder(word, n))
wordcloud2(data = freq_df2, minRotation = 0, maxRotation = 0, ellipticity = 0.8)
###You may need these
#library(wordcloud)
library(RColorBrewer)
pal <- brewer.pal(8,"Dark2")
#Get some frequency counts for each word
freq_df1 <- cleanTokens %>%
count(word, sort = TRUE) %>%
top_n(100) %>%
mutate(word = reorder(word, n))
wordcloud2(data = freq_df1, minRotation = 0, maxRotation = 0, ellipticity = 0.8)
library(widyr)
#get ngrams. You may try playing around with the value of n, n=3 , n=4
my_twts_ngram <- my_twts %>%
dplyr::select(cleanedTxt) %>%
unnest_tokens(output = paired_words,
input = cleanedTxt,
token = "ngrams",
n = 2)
#show ngrams with sorted values
my_twts_ngram %>%
count(paired_words, sort = TRUE)
library(tidyr)
#separate the paired words into two columns
my_twts_ngram <- my_twts_ngram %>%
separate(paired_words, c("word1", "word2"), sep = " ")
# filter rows where there are stop words under word 1 column and word 2 column
my_twts_filtered <- my_twts_ngram %>%
filter(!word1 %in% my_stopwords$word) %>%
filter(!word2 %in% my_stopwords$word)
# Filter out words that are not encoded in ASCII
# To see what's ASCCII, google 'ASCII table'
my_twts_filtered <- my_twts_filtered[stringi::stri_enc_isascii(my_twts_filtered$word1) &
stringi::stri_enc_isascii(my_twts_filtered$word2),]
# Sort the new bi-gram (n=2) counts:
my_words_counts <- my_twts_filtered %>%
count(word1, word2) %>%
arrange(desc(n))
head(my_words_counts)
# words occurring in pair after filtering out the stop words
head(my_twts_filtered)
# plot word network
my_words_counts %>%
filter(n >= 2) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = .6, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 4) +
labs(title = "Figure 5: Word Network: Tweets using my hashtag",
subtitle = "Text mining twitter data",
x = "", y = "")
my_twts <- search_tweets(q = "Home Park", n = 200,
lang = "en",
include_rts = FALSE)
#save the tweets downloaded using rtweet as a csv
#write_as_csv(my_twts, "my_twts.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
head(my_twts$text,3)
my_twts
?search_tweets
my_twts <- search_tweets(q = "Home Park", n = 400,
lang = "en",
include_rts = FALSE)
#save the tweets downloaded using rtweet as a csv
#write_as_csv(my_twts, "my_twts.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
head(my_twts$text,3)
my_twts
my_twts <- my_twts %>% cbind(my_twts %>% users_data(my_twts) %>% select(screen_name, location))
my_twts %>% users_data() %>% select(screen_name, location)
my_twts %>% users_data() %>% names()
my_twts %>% users_data() %>% head()
my_twts %>% users_data() %>% print(n=10, width = 10000)
my_twts
my_twts %>% users_data()
my_twts %>% users_data() %>% .$entities
my_twts <- my_twts %>% cbind(my_twts %>% users_data() %>% select(name, screen_name, location, favourites_count))
my_twts
my_twts <- search_tweets(q = "Home Park", n = 400,
lang = "en",
include_rts = FALSE)
my_twts$coordinates
head(my_twts$text,3)
head(my_twts$text)
?search_tweets
my_twts <- search_tweets(q = '"Home Park"', n = 400,
lang = "en",
include_rts = FALSE)
my_twts <- my_twts %>%
cbind(my_twts %>% users_data() %>% select(name, screen_name, location, favourites_count))
head(my_twts$text,3)
?search_tweets
my_twts <- search_tweets(q = '"이태원원"', n = 400,
lang = "en",
include_rts = FALSE)
my_twts
my_twts
my_twts <- search_tweets(q = '"이태원"', n = 400,
lang = "en",
include_rts = FALSE)
my_twts
my_twts$text
