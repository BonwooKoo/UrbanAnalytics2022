---
title: "UA2022-Yelp"
author: "Bonwoo Koo & Subhrajit Guhathakurta"
date: '2022-06-30'
output:
  rmdformats::downcute:
    downcute_theme: "chaos"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Yelp

```{r cars}
library(tidycensus)
library(sf)
library(tmap)
library(jsonlite)
library(tictoc)
library(tidyverse)
library(magrittr)
library(httr)
library(jsonlite)
library(reshape2)

setwd(eval(file.path(Sys.getenv('setwd'), 'Work/Working/School/UA_2022/external/Lab/module_1')))
```

```{r}
# Installing yelpr package requires devtools package
if ("yelpr" %in% installed.packages() == T){
  # If yelpr is already installed
  library(yelpr)
} else {
  # yelpr needs to be installed
  if ("devtools" %in% installed.packages() == F){
    # devtools is missing 
    install.packages("devtools", dependencies = T)  
  } 
    # install yelpr and load it
    devtools::install_github("OmaymaS/yelpr")
}
  

library(yelpr)
```

# Downloading Census Track polygons

Due to the limits in Yelp API in terms of how many results (i.e., rows) can be returned per each querry, it is safer to break the querry into smaller bits.

There are many strategies for doing that, and I am using Census Tracts as the 'bits'. 

The Census Bearue provides a good API, which was implemented into R through a package named *tidycensus*. To retrieve the American Community Survey data, we can use `get_acs()` function. To all functionalities in *tidyverse* package, try reading this  [official documentation](https://walker-data.com/tidycensus/). 

To use `get_acs()`, we first need to understand what arguments are needed for this function. Try `?tidycensus::get_acs` in your console to check the documentation for this function.

We will specify the following argument and leave the rest as default: geography, state, county, variables, year, survey, geometry, and output. The returned output from this function is assigned to an R object named `tract`.

# Add API Key somewhere here

```{r}
#### Tract polygons for the center of Yelp querry
tract <- get_acs(geography = "tract",
                 state = "GA",
                 county = c("Fulton", "Dekalb"),
                 variables = c(hhincome = 'B19019_001',
                               race.tot = "B02001_001", 
                               race.white = "B02001_002", 
                               race.black = 'B02001_003'
                               ),
                 year = 2019,
                 survey = "acs5",
                 geometry = TRUE,
                 output = "wide") %>% 
  select(geo.id = GEOID, 
         hhincome = hhincomeE,
         race.tot = race.totE,
         race.white = race.whiteE,
         race.black = race.blackE)

# Function: Get tract-wise radius
get_r <- function(poly){
  # Get bounding box
  bb <- st_bbox(poly) 
  # Get one point of the bb
  bb_p <- st_point(c(bb[1], bb[2])) 
  # Get centroid of the bb
  c <- st_centroid(poly) 
  # Distance between bb_p and c
  r <- st_distance(bb_p, c)
  return(r)
}

r4all <- tract %>%
  st_geometry() %>% st_transform(crs = 26967) %>% 
  sapply(., function(x) get_r(x))

tract$radius <- units::set_units(r4all,m)

tract.c <- st_centroid(tract) %>% st_transform(4326)
tract.c$x <- st_coordinates(tract.c)[,1]
tract.c$y <- st_coordinates(tract.c)[,2]
tract.c <- tract.c %>% st_set_geometry(NULL)
```

To visualize what `tract.c` means, run the following code.

```{r}
tmap_mode('view')

tract.c[1:10,] %>% 
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  st_buffer(., dist = .$radius) %>% 
  tm_shape(.) + tm_polygons(alpha = 0.5, col = 'red') +
  # Comparison to the original polygon
  tm_shape(tract[1:10,]) + tm_borders(col= 'blue') 

# Look at geo.id == '13121009502'.
# Why is the buffer a bit too large than the polygon while other tracts have correctly sized circles?
```

| Quiz: Look at GEOID = 13121009502. You will notice that the radius is slightly too large for the Census Track. Other tracks will display circles that fits the size of Census Tract much better. Why do you think this happened?


An alternative way to get a similar result is shown below. To see what the code below does, you can copy the code and run it in the console.

```{r eval=FALSE}
# Get bb for two counties
fulton_bb <- osmdata::getbb("Fulton County, GA")
dekalb_bb <- osmdata::getbb("DeKalb County, GA")

# Find coordinates for the four sides of the bb
left_x <- min(c(fulton_bb[1,],dekalb_bb[1,])) # <<-- smaller , larger(i.e., closer to zero) -->>
right_x <- max(c(fulton_bb[1,],dekalb_bb[1,]))
bottom_y <- min(c(fulton_bb[2,],dekalb_bb[2,]))
top_y <- max(c(fulton_bb[2,],dekalb_bb[2,]))

# Break the bb into a grid
fishnet_n <- 40
steps <- abs(left_x - right_x)/fishnet_n


# Fishnet points
fish_x <- seq(from = left_x, to = right_x, by = steps)
fish_y <- seq(from = bottom_y, to = top_y, by = steps)

fishnet <- expand.grid(fish_x, fish_y) %>% 
  rename(x = Var1, y = Var2) %>% 
  st_as_sf(coords = c('x', 'y'), crs = 4326)

# Visualize it
tm_shape(fishnet %>% 
           st_buffer(dist = units::set_units(steps, "degree"))) + tm_polygons(alpha = 0.2, col = 'red')

```


# Defining function for accessing Yelp API

This function below generates a fully-cleaned output. In class, we can provide students with a function that generates raw output (or close to raw) and have them clean it step by step.

After the cleaning process, we can come back to this function and modify the function to embed the cleaning process inside the function.

Mini-quiz -> how to supress messages

```{r}
#### yelp api function
yelp_search <- function(lat, lon, radius, category, api.key){
  # Pagination inddex
  count <- 1
  # Initial run (needed to get $total value)
  json <- business_search(api_key = api.key, 
                          categories = category, 
                          latitude = lat, 
                          longitude = lon, 
                          offset = (count - 1)*50, 
                          radius = radius, 
                          limit = 50) 
  # If there is no match from Yelp
  if (json$total == 0) {
    # Create an empty data.frame 
    json.all <- data.frame(id = NA, name = NA, categories = category, 
                           n.rating = NA, rating = NA, price = NA, is_closed = NA, 
                           lat = NA, lon = NA)
  # Else
  } else {
    # Convert json-like structure into data.frame format
    json.all <- json$businesses %>% 
      jsonlite::flatten() %>% # to eliminate nested structure
      mutate(price = ifelse('price' %in% names(.), price, NA), # If there is no price column
             is_closed = ifelse('is_closed' %in% names(.), is_closed, NA)) %>%  # is there is no is_close column
      select(id, name, categories, 
             n.rating = review_count, 
             rating, price, is_closed, 
             lat = coordinates.latitude, 
             lon = coordinates.longitude) # select & name changes
    
    # Drop rows that have zero entries in category column
    #####################################################
    # Change the code below to simplify it

    # Unlist the category column and create a character string seperated by comma
    # <- This function 'melt' emits many messages. Suppressing them.
    
    cate.title <- suppressMessages({
      reshape2::melt(json.all$categories)
      }) %>% 
      group_by(L1) %>% 
      summarize(categories = paste0(title, collapse = ", "))
    
    
    # Replace category column in the original table with the cleaned one
    json.all$categories <- cate.title$categories
    
    # Get how many querries are required to loop through all matches
    offset_n <- ceiling(json$total / 50)
    
    # Start looping
    while(offset_n > count){
      count <- count + 1
      if (offset_n != count){ # For all loop before the final one
        json <- business_search(api_key = api.key, 
                                categories = category, 
                                latitude = lat, 
                                longitude = lon, 
                                offset = (count - 1)*50, 
                                radius = radius, 
                                limit = 50)
        
      } else { # The final one
        json <- business_search(api_key = api.key, 
                                categories = category, 
                                latitude = lat, 
                                longitude = lon, 
                                offset = (count - 1)*50, 
                                radius = radius, 
                                limit = json$total - 50*(count - 1))
        
      }
      
      # Convert json-like structure into data.frame format
      json.cont <- json$businesses %>% 
        mutate(price = ifelse('price' %in% names(.), price, NA),
               is_closed = ifelse('is_closed' %in% names(.), is_closed, NA)) %>% 
        jsonlite::flatten() %>% select(id, name, categories, 
                                       n.rating = review_count, 
                                       rating, price, is_closed, 
                                       lat = coordinates.latitude, 
                                       lon = coordinates.longitude)
      
      # Some rows do not have category names. It is okay to keep them, 
      # but for the purpose of exercise, let's try dropping such rows.
      category_filter <- map_lgl(json.cont$categories, function(x) length(x[[1]]) != 0)
      
      json.cont <- json.cont[category_filter,]
      
      cate.title <- suppressMessages({
        reshape2::melt(json.cont$categories)
        }) %>% 
        group_by(L1) %>% 
        summarize(categories = paste0(title, collapse = ", "))
      
      json.cont$categories <- cate.title$categories
      
      # This version of code has redundancy. I need to organize it such that 
      # this cleaning process is done once.
      json.all <- bind_rows(json.all, json.cont)
    }
  }
  
  return(json.all)
}
```


```{r}
#### data download 
# API key
api.key <- Sys.getenv('yelp_api') # To Subhro, to use this code, you need to edit your system envirnnment variables. See https://www.howtogeek.com/787217/how-to-edit-environment-variables-on-windows-10-or-11/#:~:text=In%20the%20System%20Properties%20window,%2C%20and%20click%20%E2%80%9COK.%E2%80%9D

# Receiving DF
result.temp <- data.frame(geo.id = integer(0), 
                          id = integer(0), 
                          name = integer(0), 
                          categories = integer(0),
                          n.rating = integer(0), 
                          rating = integer(0), 
                          price = integer(0), 
                          is_closed = integer(0), 
                          lat = integer(0), 
                          lon = integer(0))

# Read yelp.csv if it already exists in your drive
if ("yelp.csv" %in% list.files(getwd())){
  yelp <- read.csv("yelp.csv")
} else {
  # If the file doesn't exist, get it from the Yelp server
  for (i in 1:nrow(tract.c)) {
    temp <- tract.c[i,]
    lat <- temp$y
    lon <- temp$x
    radius <- round(as.numeric(temp$radius))
    json <- yelp_search(lat, lon, radius, 'restaurant', 
                        api.key) %>% 
      mutate(geo.id = temp$geo.id)
    result.temp <- result.temp %>% rbind(json)
    print(paste0("Done with ", i))
  }
  
  # rm(i, json, lat, lon, api.key, radius, temp, tract.c, yelp_search)
  write.csv(result.temp, "yelp.csv")
}

```


*Done!*


# Doing it from the scratch

Shown below is the raw output from the `business_search()` in `yelpr` package.

*I first need to identify what sorts of clean can be done to this file.* If there are other types of cleaning, I will need to show students how to trouble shoot. I might have students to the Googling themselves and have them go through the process themselves in the classroom.

```{r}
# In case you want to see the raw query 
test <- business_search(api_key = Sys.getenv('yelp_api'),
                        category = 'restaurant',
                        latitude = 33.74876295437723,
                        longitude = -84.39073773089834,
                        offset = 1,
                        radius = 1000,
                        limit = 50)
```

```{r}
test_rest <- yelp_search(lat = 33.74876295437723,
                         lon = -84.39073773089834,
                         radius = 1000,
                         category = 'restaurant',
                         api.key = Sys.getenv('yelp_api'))
```

# Cleaning process

## What issues do we see in the raw download?
[API-related issues]
- Pagination.
- Try `class(test)`. What is this that we get from `business_search()` function?
  - test running `jsonlite::flatten(test$businesses) %>% select(coordinates.latitude, coordinates.longitude)` and `test$businesses %>% select(coordinates.latitude, coordinates.longitude)`. The latter will not run.
- Nested data.frame structure
- Inconsistent return columns

[Data cleaning]
- Closed business.
- Missing values.
- Unintuitive values, such as \$ and \$\$ for the price level.
- Category column is not really useful


```{r}
## Saving the R environment
# save.image("D:/Dropbox (GaTech)/Work/Working/School/UA_2022/Lab/W3/yelp_test.RData")
```

# Merging the cleaned data with Census data

One of the most commonly used and basic source of data on wide array of different aspects of life is, of course, the Census data. It is incredibly useful if you can 'join' one geospatial data to census data.

### Converting the downloaded Yelp data to a geospatial data

```{r}
# Converting the POI result into SF
result.sf <- result.temp %>% 
  filter(!is.na(lat) | !is.na(lon)) %>% 
  st_as_sf(coords = c("lon", "lat"), dim = "XY", crs = 4326)
```


### Spatial join the two data
```{r}
# Spatial join the SF to Census data
result.census <- result.sf %>% 
  st_join(tract %>% st_transform(crs = 4326), st_intersects)

# result.census %>% 
#   st_write("D:/Dropbox (GaTech)/Work/Working/School/UA_2022/Lab/W3/restaurant_census.geojson")
```

```{r}
# Visualize the map
tmap_mode('view')
tm_shape(result.census) + tm_dots(col = "hhincome")
```

