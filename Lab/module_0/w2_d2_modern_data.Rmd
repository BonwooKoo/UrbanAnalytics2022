---
title: "Data for Urban Analytics"
subtitle: ""
author: "Bon Woo Koo & Subhro Guhathakurta"
institute: "Georgia Institute of Technology"
date: "2022/8/30"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    seal: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '4:3' # alternatives '16:9' or '4:3' or others e.g. 13:9
---
class: middle, inverse

# Data for Urban Analytics

.font100[
Bon Woo Koo & Subhro Guhathakurta

8/30/2022
]

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Open Sans"),
  text_font_google   = google_font("Source Sans Pro", "400", "400i"),
  code_font_google   = google_font("Fira")
)

nice_table <- function(df, height="150px"){
  return(
    df %>% kable() %>% 
      kable_styling(latex_options="scale_down", font_size=12) %>% 
      scroll_box(width = "100%", height = height)
    )
}
```

```{r, include=F}
library(xaringanthemer)
library(sf)
library(tidyverse)
library(leaflet)
library(tmap)
library(kableExtra)
```

---


## Data for Urban Analytics

**Big data is not just about size.** If the size was all that makes a data big, then there is nothing new about big data; we've had it for a long time. For example,

* National Decennial Census <br> .footnotesize[.gray[(which technically covers the entire population; hundreds of millions)]]
* Survey response for electoral predictions <br> 
  .footnotesize[.gray[(e.g., The famous 2.4 million survey responses by Literary Digest in 1936, which, despite the size, miserably failed)]]

Also, **size per observation** can be very different (e.g., text vs. image).

.center[
### The characteristics of modern data that gave rise to urban analytics is more complex.
]
---
## Characteristics of Big Data

Kitchin (2014) details that big data can be characterized by: 

.footnotesize[
* **Volume**, consisting of terabytes or petabytes of data;  
.scriptsize[.gray[Walmart generates 2.5 petabyte of transaction data **every hour** (Kitchin & McArdle, 2014).]]

* **Velocity**, being created in or near real-time &rarr; .red[**Key trait of big data, according to Kitchin**];  
.scriptsize[.gray[Note, however, that frequency of **generation** != **publishing** (Kitchin & McArdle, 2014).]]

* **Variety**, being structured and unstructured in nature;  
.scriptsize[.gray[Unstructured data were used in the past, but **not at the scale we use them now**.]]
]

--
.footnotesize[
And goes on to include:

* **Exhaustive** in scope, striving to capture entire populations or systems (n=all) &rarr; .red[**Another key trait of big data, according to Kitchin**];  
.scriptsize[.gray[Twitter data contains the entirety of the Twitter users. The definition of "all" can be relative.]]

* fine-grained in **resolution**, aiming to be as detailed as possible;

* **relational** in nature, containing common fields that can join different data sets;

* **flexible** (can add new fields easily) and **scaleability** (can expand in size rapidly).
]
---
##  Challenges that Big Data introduces

* **Volume** &#8594; Need for more efficient algorithms or more powerful computers. Can't use Excel. <font size=3px, color="grey"> (capped at 1 million rows by ~16K columns; [source](https://support.microsoft.com/en-us/office/excel-specifications-and-limits-1672b34d-7043-467e-8e27-269d656771c3)) </font>

--

* **Velocity** &#8594; Need for more direct & automated access to the data storage (e.g., API). Analytics also need to update models frequently to reflect the incoming stream of data.

--

* **Variety** &#8594;  Need for new analytical techniques to convert unstructured data into structured formats. <br> 
.footnotesize[.gray[*E.g., having all the street view images in the world won't do much good if there was no automated image processing. We can't insert images into a regression analysis.*]]

--

.center[
<font size=6px, color='darkgreen'> These challenges require that we combine <br>"innovative statistical methods, novel comptuer science, and original theories" (King, 2016, pp. ix-x) </font>
]

---
## Sources of Big Data

* **Directed** &rarr; "Generated by traditional forms of surveillance, wherein the gaze of the technology is focused on a person or place by a human operator" (Kitchin 2014, p.4). Satellite and LiDAR images,  Google Street View images. 

* **Automated** &rarr; "Generated as an inherent, automatic function of the device or system" (Kitchin 2014, p.4). GPS from cell phones, transaction records and clickstream on e-commerce websites such as Amazon, and tap-in and tap-out records from public transportation  systems. 

* **Volunteered** &rarr; Generated by users. Postings on social media such as Twitter, Facebook and Reddit, images and videos uploaded by users, reviews and tips left on Google Places and Yelp, and user-contributed large databases such as OpenStreetMap. 

<!-- --- -->
<!-- Many of these big data are unstructured. Only about 5 percent of all existing data are -->
<!-- structured (that is, tabular data in a spreadsheet or similar formats) while the rest is not -->
<!-- in these formats (Cukier 2010; Gandomi and Haider 2015) -->

<!-- Unstructured data, such as -->
<!-- images, audio, video and unstructured texts, often need to be translated to into structured -->
<!-- formats required by analysis and modelling conventions (Gandomi and Haider 2015).  -->

<!-- Many data coming from sensors and wireless networks (for example, smartphones) are -->
<!-- inherently spatial and spatiotemporal (Jardak et al. 2014). A study in 2012 noted that Google generates -->
<!-- about 25 petabytes of data per day, and a significant portion of the data has spatiotemporal components (Vatsavai et al. 2012). -->

<!-- In addition to relational data, the spatial dimension -->
<!-- of big data offers important insights and allows researchers to gain greater value from the -->
<!-- data by, for example, joining different datasets that are otherwise disconnected. -->

---
## Are we abandoning small data?

* **Certainly not.** There are many caveats in Big Data and we are still learning what those are.

  * When they say n=all, is "all" really the entire population of the US?  
  * If not, does "all" equates with your population of interest?  
  * Does having "all" observations provide more accurate findings?

* Small data studies will continue to be valuable because of their utility in answering targeted queries (Kitchin, 2015, 463).

* Best approach at the moment is to wisely merge Big and small data sets.

* Be mindful of the **unit/frequency in which data are measured**: When merging data sets, one that has more small data-like quality often determines the output of the merge.  
.footnotesize[.gray[E.g., joining coordinate-level data with Census Tract-level data, merging real-time feed with quarterly data]]
